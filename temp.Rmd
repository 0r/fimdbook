## Statistical intervals and tests {#sec:inference}

### Scalar or multi-parameter inference?

The ultimate objective of multiple imputation is to provide valid
statistical estimates from incomplete data. For scalar $Q$, it is
straightforward to calculate confidence intervals and $p$-values from
multiply imputed data, the primary difficulty being the derivation of
the appropriate degrees of freedom for the $t$- and $F$-distributions .
Section \@ref(sec:singlepar) provides the relevant statistical procedures.

If $Q$ is a vector, we have two options for analysis. The first option
is to calculate confidence intervals and $p$-values for the individual
elements in $Q$, and do all statistical tests per element. Such
repeated-scalar inference is appropriate if we interpret each element as
a separate, though perhaps related, model parameter. In this case, the
test uses the fraction of missing information particular to each
parameter.

The alternative option is to perform one statistical test that involves
the elements of $Q$ at once. This is appropriate in the context of
multi-parameter or simultaneous inference, where we evaluate
combinations of model parameters. Practical applications of such tests
include the comparison of nested models and the testing of model terms
that involved multiple parameters like regression estimates for dummy
codings created from the same variable.

All methods assume that, under repeated sampling and with complete data,
the parameter estimates $\hat Q$ are normally distributed around the
population value $Q$ as $$\hat Q \sim N(Q, U)$$ where $U$ is the
variance-covariance matrix of $(Q-\hat Q)$ [@RUBIN1987 p. 75]. For
scalar $Q$, the quantity $U$ reduces to $\sigma_m^2$, the variance of
the estimate $\hat Q$ over repeated samples. Observe that $U$ is not the
variance of the measurements.

Several approaches for multi-parameter inference are available: Wald
test, likelihood ratio test and $\chi^2$-test. These methods are more
complex than single-parameter inference, and their treatment is
therefore deferred to Section \@ref(sec:pooling). The next section shows
how confidence intervals and $p$-values for scalar parameters can be
calculated from multiply imputed data.

### Scalar inference {#sec:singlepar}

Single parameter inference applies if $k=1$, or if $k>1$ and the test is
repeated for each of the $k$ components. Since the total variance of $T$
is not known a priori, $\bar Q$ follows a $t$-distribution rather than
the normal. Univariate tests are based on the approximation
$$\frac{Q-\bar Q}{\sqrt{T}} \sim t_\nu$$ where $t_\nu$ is the Student’s
$t$-distribution with $\nu$ degrees of freedom, with $\nu$ defined by
Equation \@ref(eq:newnu).

The $100(1-\alpha)$% confidence interval of a $\bar Q$ is calculated as
$$\bar Q \pm t_{\nu,1-\alpha/2}\sqrt{T}$$ where $t_{\nu,1-\alpha/2}$ is
the quantile corresponding to probability $1-\alpha/2$ of $t_\nu$. For
example, use $t_{10,0.975}=2.23$ for the 95% confidence interval with
$\nu=10$.

Suppose we test the null hypothesis $Q=Q_0$ for some specified value
$Q_0$. We can find the $p$-value of the test as the probability
$$P_s = \Pr\left[F_{1,\nu} > \frac{(Q_0 - \bar Q)^2}{T}\right]$$ where
$F_{1,\nu}$ is an $F$ where $F_{1,\nu}$ is an $F$-distribution with 1
and $\nu$ degrees of freedom.

### Numerical example

Wald tests and confidence intervals for individual elements of $Q$ are
standard output of most statistical procedures. The
`mice` package provides such
output by running the `summary()` function on
the `mipo` object created by
`pool()`:

`   `

                estimate std.error statistic   df        p.value
    (Intercept)    30.50      2.24     13.63 12.4 0.000000000694
    age            -2.13      1.08     -1.97 15.1 0.067575739870
                2.5 % 97.5 %
    (Intercept) 25.65 35.362
    age         -4.43  0.174

The `estimate` and
`df` columns are identical to the previous
display. In addition, we get the standard error of the estimate, the
Wald statistics, its associated $p$-value, and the nominal 95th percent
confidence interval per parameter. In this toy example
`age` is not a statistically significant
predictor of `bmi` at a type I error rate of 5
percent. We may change the nominal length of the confidence intervals by
the `conf.level` argument. It is possible to
obtain all output by
`summary(est, all, conf.int = TRUE)`.

## How to evaluate imputation methods {#sec:evaluation}

### Simulation designs and performance measures

The advantageous properties of multiple imputation are only guaranteed
if the imputation method used to create the missing data is proper.
Equations \@ref(eq:proper1)–
\@ref(eq:proper3) describe the conditions needed
for proper imputation.

Checking the validity of statistical procedures is often done by
simulation. There are generally two mechanisms that influence the
observed data, the sampling mechanism and the missing data mechanism.
Simulation can address sampling mechanism separately, the missing data
mechanism separately, and both mechanisms combined. This leads to three
general simulation designs.

1.  *Sampling mechanism only*. The basic simulation steps are:
    choose $Q$, take samples $Y^{(s)}$, fit the complete-data model,
    estimate $\hat Q^{(s)}$ and $U^{(s)}$ and calculate the outcomes
    aggregated over $s$.

2.  *Sampling and missing data mechanisms combined*. The basic
    simulation steps are: choose $Q$, take samples $Y^{(s)}$, generate
    incomplete data $Y_{\mathrm obs}^{(s,t)}$, impute, estimate
    $\hat Q^{(s,t)}$ and $T^{(s,t)}$ and calculate outcomes aggregated
    over $s$ and $t$.

3.  *Missing data mechanism only*. The basic simulation steps are:
    choose $(\hat Q, U)$, generate incomplete data
    $Y_{\mathrm obs}^{(t)}$, impute, estimate $(\bar Q, \bar U)^{(t)}$
    and $B^{(t)}$ and calculate outcomes aggregated over $t$.

A popular procedure for testing missing-data applications is design 2
with settings $s=1,\dots,1000$ and $t = 1$. As this design does not
separate the two mechanisms, any problems found may result from both the
sampling and the missing-data mechanism. Design 1 does not address the
missing data, and is primarily of interest to study whether any problems
are attributable to the complete-data model. Design 3 addresses the
missing-data mechanism only, and thus allows for a more detailed
assessment of any problem caused by the imputation step. An advantage of
this procedure is that no population model is needed. @BRAND2003
describe this procedure in more detail.

If we are primarily interested in determining the quality of imputation
methods, we may simplify evaluation by defining the sample equal to the
population, and set the within-variance $\bar U = 0$ in Equation
\@ref(eq:poolT). See @VINK2014 for a short exploration of the idea.

### Evaluation criteria {#sec:evaluationcriteria}

The goal of multiple imputation is to obtain statistically valid
inferences from incomplete data. The quality of the imputation method
should thus be evaluated with respect to this goal. There are several
measures that may inform us about the statistical validity of a
particular procedure. These are:

1.  *Raw bias (RB) and percent bias (PB)*. The raw bias of the
    estimate $\bar Q$ is defined as the difference between the expected
    value of the estimate and truth: $\rm{RB} = \rm{E}(\bar Q) - Q$. RB
    should be close to zero. Bias can also be expressed as percent bias:
    $\rm{PB} = 100 \times |(\rm{E}(\bar Q) - Q) / Q|$. For acceptable
    performance we use an upper limit for PB of 5%.
    [@DEMIRTAS2008C]

2.  *Coverage rate (CR)*. The coverage rate (CR) is the proportion
    of confidence intervals that contain the true value. The actual rate
    should be equal to or exceed the nominal rate. If CR falls below the
    nominal rate, the method is too optimistic, leading to
    false positives. A CR below 90 percent for a nominal 95 percent
    interval indicates poor quality. A high CR (e.g., 0.99) may indicate
    that confidence interval is too wide, so the method is inefficient
    and leads to inferences that are too conservative. Inferences that
    are “too conservative” are generally regarded a lesser sin than “too
    optimistic”.

3.  *Average width (AW)*. The average width of the confidence
    interval is an indicator of statistical efficiency. The length
    should be as small as possible, but not so small that the CR will
    fall below the nominal level.

4.  *Root mean squared error (RMSE)*. The
    $\rm{RMSE} = \sqrt{(\rm{E}(\bar Q) - Q)^2}$ is a compromise between
    bias and variance, and evaluates $\bar Q$ on both accuracy and
    precision.

If all is well, then RB should be close to zero, and the coverage should
be near 0.95. Methods having no bias and proper coverage are called
randomization-valid [@RUBIN1987]. If two methods are both
randomization-valid, the method with the shorter confidence intervals is
more efficient. While the RMSE is widely used, we will see in Section
\@ref(sec:true) that it is not a suitable metric to evaluate multiple
imputation methods.

### Example {#sec:quantifyingbias}

This section demonstrates the measures defined in Section
\@ref(sec:evaluationcriteria) can be calculated using simulation. The
process starts by specifying a model that is of scientific interest and
that fixes $Q$. Pseudo-observations according to the model are
generated, and part of these observations is deleted, resulting in an
incomplete dataset. The missing values are then filled using the new
imputation procedure, and Rubin’s rules are applied to calculate the
estimates $\bar Q$ and $T$. The whole process is repeated a large number
of times, say in 1000 runs, each starting from different random seeds.

For the sake of simplicity, suppose scientific interest focuses on
determining $\beta$ in the linear model
$y_i = \alpha + x_i \beta + \epsilon_i$. Here
$\epsilon_i \sim N(0, \sigma^2)$ are random errors uncorrelated with
$x$. Suppose that the true values are $\alpha = 0$, $\beta = 1$ and
$\sigma^2 = 1$. We have 50% random missing data in $x$, and compare two
imputation methods: regression imputation (cf. Section \@ref(sec:regimp))
and stochastic regression imputation (cf. Section \@ref(sec:sri)).

It is convenient to create a series of small
`R` functions. The
`create.data()` function randomly draws
artificial data from the specified linear model.

`          `\
`                          ``) {`\
`   `\
`    `\
`           `\
`     `\

Next, we remove some data in order to make the data incomplete. Here we
use a simple random missing data mechanism (MCAR) to generate
approximately 50% missing values.

`     ``){`\
`     `\
`       `\
`  `\

We then define a small test function that calls
`mice()` and applies Rubin’s rules to the
imputed data.

`         ``) {`\
`           `\
`       `\
`        `\
`    `“`2.5 %`”` `“`97.5 %`”\

The following function puts everything together:

`    ``) {`\
`        `\
`     `\
`                        `\
`                         `“`2.5 %`”“`97.5 %`”\
`    ``runs) {`\
`       `\
`      `\
`         `\
`                                   `\
`         `\
`  `\
`  `\

Performing 1000 simulations is now done by calling
`simulate()`, thus

`  `

The means of the estimate, the lower and upper bounds of the confidence
intervals per method, can be obtained by

`     `

                 estimate 2.5 % 97.5 %
    norm.predict    1.343 1.065   1.62
    norm.nob        0.995 0.648   1.34

The function of the following code is to calculate the quality
statistics as defined in Section \@ref(sec:evaluationcriteria).

`  `\
`     `\
`        `\
`   `“`2.5 %`”`       `“`97.5 %`”\
`   `“`97.5 %`”`   `“`2.5 %`”\
`     `\

                     RB   PB    CR    AW  RMSE
    norm.predict  0.343 34.3 0.364 0.555 0.409
    norm.nob     -0.005  0.5 0.925 0.693 0.201

The interpretation of the results is as follows. Regression imputation
by method `norm.predict` produces severely
biased estimates of $\beta$. The true $\beta$ is 1, but the average
estimate after regression imputation is 1.343. Moreover, the true value
is located within the confidence interval in only 36% of the cases, far
below the nominal value of 95%. Hence, regression imputation is not
randomization-valid for $\beta$, even under MCAR. Because of this, the
estimates for AW and RMSE are not relevant. This example shows that
statistical inference on incomplete data that were imputed by regression
imputation can produce the wrong answer.

The story for stochastic regression imputation is different. The
`norm.nob` method is unbiased and has a
coverage of 92.5%. The method is not randomization-valid, but it is
near. The AW and RMSE serve as useful indicators, though both may be a
little low as the confidence intervals appear somewhat short. Chapter
\@ref(ch:univariate) shows how to make it fully randomization-valid.

Of course, simulation studies do not guarantee fitness for a particular
application. However, if simulation studies illustrate limitations in
simple examples, we may expect these will also be present in
applications.

## Imputation is not prediction {#sec:true}

In the world of simulation we have access to both the true and imputed
values, so an obvious way to quantify the quality of a method is to see
how well it can recreate the true data. The method that best recovers
the true data “wins.” An early paper developing this idea is
@GLEASON1975, but the literature is full of examples. The approach is
simple and appealing. But will it also select the best imputation
method?

The answer is “No”. Suppose that we would measure discrepancy by the
RMSE of the imputed values:
$$\mathrm{RMSE} = \sqrt{\frac{1}{n_\mathrm{mis}}\sum_{i=1}^{n_\mathrm{mis}} ({\mbox{$y_i^\mathrm{mis}$}}- {\mbox{$\dot y_i$}})^2} (\#eq:recovery)$$
where $y_i^\mathrm{mis}$ represents the true (removed) data value for
unit $i$ and where $\dot y_i$ is imputed value for unit $i$. For
multiply imputed data we calculate RMSE for each imputed dataset, and
average these.

It is well known that the minimum RMSE is attained by predicting the
missing $\dot y_i$ by the linear model with the regression weights set
to their least squares estimates. According to this reasoning the “best”
method replaces each missing value by its most likely value under the
model. However, this will find the same values over and over, and is
single imputation. This method ignores the inherent uncertainty of the
missing values (and acts as if they were known after all), resulting in
biased estimates and invalid statistical inferences. Hence, the method
yielding the lowest RMSE is bad for imputation. More generally, measures
based on similarity between the true and imputed values do not separate
valid from invalid imputation methods.

Let us check this claim with a short simulation. The
`rmse()` below calculates the RMSE from the
true and multiply imputed data for missing data in variable
`x`.

`      ``) {`\
`     `\
`    `\
`     ``m)) {`\
`      `\
`      `\
`        `\
`  `\
`  `\

The `simulate2()` function creates the same
missing data as before.

`    ``) {`\
`        `\
`     `\
`                        `\
`                        `\
`    ``runs) {`\
`       `\
`      `\
`            `\
`                  `\
`      `\
`            `\
`      `\
`  `\
`  `\

`  `\
`     `

                  RMSE
    norm.predict 0.725
    norm.nob     1.025

The simulation confirms that regression imputation is better at
recreating the missing data. Remember from Section \@ref(sec:regimp) that
regression imputation is fundamentally flawed. Its estimate of $\beta$
is biased (even under MCAR) and the accompanying confidence interval is
too short.

The example demonstrates that the RMSE is not informative for evaluating
imputation methods. Assessing the discrepancy between true data and the
imputed data may seem a simple and attractive way to select the best
imputation method. However, it is not useful to evaluate methods solely
based on their ability to recreate the true data. On the contrary,
selecting such methods may be harmful as these might increase the rate
of false positives. Imputation is not prediction.

## When not to use multiple imputation {#sec:when}

Should we always use multiple imputation for the missing data? We
probably could, but there are good alternatives in some situations.
Section \@ref(sec:doesnotcover) already discussed some approaches not
covered in this book, each of which has its merits. This section
revisits complete-case analysis. Apart from being simple to apply, it
can be a viable alternative to multiple imputation in particular
situations.

Suppose that the complete-data model is a regression with outcome $Y$
and predictors $X$. If the missing data occur in $Y$ only, complete-case
analysis and multiple imputation are equivalent, so then complete-case
analysis is preferred since it is easier, more efficient and more robust
[@VONHIPPEL2007]. This applies to the regression weights. Quantities
that depend on the correct marginal distribution of $Y$, such as the
mean or $R^2$, require the stronger MCAR assumption. Multiple imputation
gains an advantage over complete-case analysis if additional predictors
for $Y$ are available that are not part of $X$. The efficiency of
complete-case analysis declines if $X$ contains missing values, which
may result in inflated type II error rates. Complete-case analysis can
perform quite badly under MAR and some MNAR cases [@SCHAFER2002], but
there are two special cases where listwise deletation outperforms
multiple imputation.

The first special case occurs if the probability to be missing does not
depend on $Y$. Under the assumption that the complete-data model is
correct, the regression coefficients are free of bias
[@LITTLE1992; @KING2001]. This holds for any type of regression
analysis, and for missing data in both $Y$ and $X$. Since the missing
data rate may depend on $X$, complete-case analysis will in fact work in
a relevant class of MNAR models. @WHITE2010C confirmed the superiority
of complete-case analysis by simulation. The differences were often
small, and multiple imputation gained the upper hand as more predictive
variables were included. The property is useful though in practice.

The second special case holds only if the complete data model is
logistic regression. Suppose that the missing data are confined to
either a dichotomous $Y$ or to $X$, but not to both. Assuming that the
model is correctly specified, the regression coefficients (except the
intercept) from the complete-case analysis are unbiased if the
probability to be missing depends only on $Y$ and not on $X$
[@VACH1994]. This property provides the statistical basis of the
estimation of the odds ratio from case-control studies in epidemiology.
If missing data occur in both $Y$ and $X$ the property does not hold.

At a minimum, application of listwise deletion should be a conscious
decision of the analyst, and should preferably be accompanied by an
explicit statement that the missing data fit in one of the three
categories described above.

Other alternatives to multiple imputation were briefly reviewed in
Section \@ref(sec:doesnotcover), and may work well in particular
applications. However, none of these is as general as multiple
imputation.

## How many imputations? {#sec:howmany}

One of the distinct advantages of multiple imputation is that it can
produce unbiased estimates with correct confidence intervals with a low
number of imputed datasets, even as low as $m=2$. Multiple imputation is
able to work with low $m$ since it enlarges the between-imputation
variance $B$ by a factor $1/m$ before calculating the total variance in
$T=\bar U+(1+m^{-1})B$.

The classic advice is to use a low number of imputation, somewhere
between 3 and 5 for moderate amounts of missing information. Several
authors investigated the influence of $m$ on various aspects of the
results. The picture emerging from this work is that it is often
beneficial to set $m$ higher, somewhere in the range of 20–100
imputations. This section reviews the relevant work in the area.

The advice for low $m$ rests on the following argument. Multiple
imputation is a simulation technique, and hence $\bar Q$ and its
variance estimate $T$ are subject to simulation error. Setting
$m=\infty$ causes all simulation error to disappear, so $T_{\infty}<T_m$
if $m<\infty$. The question is when $T_{\infty}$ is close enough to
$T_m$. [@RUBIN1987 p. 114] showed that the two variances are related by
$$T_m=\left(1+\frac{\gamma_0}{m}\right)T_{\infty}$$ where $\gamma_0$ is
the (true) population fraction of missing information. This quantity is
equal to the expected fraction of observations missing if $Y$ is a
single variable without covariates, and commonly less than this if there
are covariates that predict $Y$. For example, for $\gamma_0=0.3$ (e.g.,
a single variable with 30% missing) and $m=5$ we find that the
calculated variance $T_m$ is $1+0.3/5 = 1.06$ times (i.e., 6%) larger
than the ideal variance $T_{\infty}$. The corresponding confidence
interval would thus be $\sqrt{1.06}=1.03$ (i.e., 3%) longer than the
ideal confidence interval based on $m=\infty$. Increasing $m$ to 10 or
20 would bring the factor down 1.5% and 0.7%, respectively. The argument
is that “the additional resources that would be required to create and
store more than a few imputations would not be well spent” [@SCHAFER1997
p. 107], and “in most situations there is simply little advantage to
producing and analyzing more than a few imputed datasets” [@SCHAFER1998
p. 549].

@ROYSTON2004B observed that the length of the confidence interval also
depends on $\nu$, and thus on $m$ (cf. Equation \@ref(eq:oldnu)). He
suggested to base the criterion for $m$ on the confidence coefficient
$t_{\nu}\sqrt{T}$, and proposed that the coefficient of variation of
$\ln(t_{\nu}\sqrt{T})$ should be smaller than 0.05. This effectively
constrains the range of uncertainty about the confidence interval to
roughly within 10%. This rule requires $m$ to be “at least 20 and
possibly more.”

@GRAHAM2007 investigated the effect of $m$ on the statistical power of a
test for detecting a small effect size $(< 0.1)$. Their advice is to set
$m$ high in applications where high statistical power is needed. For
example, for $\gamma_0=0.3$ and $m=5$ the statistical power obtained is
73.1% instead of the theoretical value of 78.4%. We need $m=20$ to
increase the power to 78.2%. In order to have an attained power within
1% of the theoretical power, then for fractions of missing information
$\gamma$ = (0.1, 0.3, 0.5, 0.7, 0.9) we need to set $m$ = (20, 20, 40,
100, $> 100$), respectively.

@BODNER2008 explored the variability of three quantities under various
$m$: the width of the 95% confidence interval, the $p$-value, and
$\gamma_0$. Bodner selected $m$ such that the width of the 95%
confidence interval is within 10% of its true value 95% of the time. For
$\gamma_0$ = (0.05, 0.1, 0.2, 0.3, 0.5, 0.7, 0.9), he recommends $m$ =
(3, 6, 12, 24, 59, 114, 258), respectively, using a linear rule. Since
the true $\gamma_0$ is unknown, Bodner suggested the proportion of
complete cases as a conservative estimate of $\gamma_0$. @VONHIPPEL2018
showed that a relation between $m$ and $\gamma_0$ is better explained by
a quadratic rule $$(\#eq:hippel)
m = 1 + \frac{1}{2} \left(
         \frac{\gamma_0}
              {\mathrm{SD}(\sqrt{U_\ell})\mathrm{E}(\sqrt{U_\ell})}
              \right)^2,$$ where $\mathrm{E}(\sqrt{U_\ell})$ and
$\mathrm{SD}(\sqrt{U_\ell})$ are the mean and standard deviation of the
standard errors calculated from the imputed datasets. The rule is used
in a two-step procedure, where the first step estimates $\gamma_0$ and
its 95% confidence interval. The upper limit of the confidence interval
is then plugged into Equation \@ref(eq:hippel). Compared to Bodner, the
rule suggests somewhat lower $m$ if $\gamma_0 < 0.5$ and substantially
higher $m$ if $\gamma_0 > 0.5$.

The starting point of @WHITE2011 is that all essential quantities in the
analysis should be reproducible within some limit, including confidence
intervals, $p$-values and estimates of the fraction of missing
information. They take a quote from @VONHIPPEL2009 as a rule of thumb:
*the number of imputations should be similar to the percentage of cases
that are incomplete*. This rule applies to fractions of missing
information of up to 0.5. If $m \approx\, 100\lambda$, the following
properties will hold for a parameter $\beta$:

1.  The Monte Carlo error of $\hat\beta$ is approximately 10% of
    its standard error;

2.  The Monte Carlo error of the test statistic
    $\hat\beta/\mathrm{se}(\hat\beta)$ is approximately 0.1;

3.  The Monte Carlo error of the $p$-value is approximately 0.01
    when the true $p$-value is 0.05.

@WHITE2011 suggest these criteria provide an adequate level of
reproducibility in practice. The idea of reproducibility is sensible,
the rule is simple to apply, so there is much to commend it. The rule
has now become the de-facto standard, especially in medical
applications. One potential difficulty might be that the percentage of
complete cases is sensitive to the number of variables in the data. If
we extend the active dataset by adding more variables, then the
percentage of complete cases can only drop. An alternative would be to
use the average missing data rate as a less conservative estimate.

Theoretically it is always better to use higher $m$, but this involves
more computation and storage. Setting $m$ very high (say $m = 200$) may
be useful for low-level estimands that are very uncertain, and for which
we want to approximate the full distribution, or for parameters that are
notoriously different to estimates, like variance components. On the
other hand, setting $m$ high may not be worth the extra wait if the
primary interest is on the point estimates (and not on standard errors,
$p$-values, and so on). In that case using $m = 5-20$ will be enough
under moderate missingness.

Imputing a dataset in practice often involves trial and error to adapt
and refine the imputation model. Such initial explorations do not
require large $m$. It is convenient to set $m=5$ during model building,
and increase $m$ only after being satisfied with the model for the
“final” round of imputation. So if calculation is not prohibitive, we
may set $m$ to the average percentage of missing data. The substantive
conclusions are unlikely to change as a result of raising $m$ beyond
$m=5$.

## Exercises {#sec:exmi}

1.  *Nomogram*. Construct a graphic representation of Equation
    \@ref(eq:gammamb) that allows the user to convert $\lambda$ and
    $\gamma$ for different values of $\nu$. What influence does $\nu$
    have on the relation between $\lambda$ and $\gamma$?

2.  *Models*. Explain the difference between the response model
    and the imputation model.

3.  *Listwise deletion*. In the `airquality`
    data, predict `Ozone` from
    `Wind` and
    `Temp`. Now randomly delete the half of
    the wind data above 10 mph, and randomly delete half of the
    temperature data above 80$^\circ$F.

    1.  Are the data MCAR, MAR or MNAR?

    2.  Refit the model under listwise deletion. Do you notice a
        change in the estimates? What happens to the standard
        errors?

    3.  Would you conclude that listwise deletion provides valid
        results here?

    4.  If you add a quadratic term to the model, would that alter
        your conclusion?

    \@ref(ex:listwise)

4.  *Number of imputations*. Consider the
    `nhanes` dataset in
    `mice`.

    1.  Use the functions `ccn()` to calculate
        the number of complete cases. What percentage of the cases is
        incomplete?

    2.  Impute the data with `mice` using the
        defaults with `seed=1`, predict
        `bmi` from
        `age`, `hyp`
        and `chl` by the normal linear
        regression model, and pool the results. What are the proportions
        of variance due to the missing data for each parameter? Which
        parameters appear to be most affected by the nonresponse?

    3.  Repeat the analysis for `seed=2` and
        `seed=3`. Do the conclusions remain
        the same?

    4.  Repeat the analysis with $m=50$ with the same seeds. Would you
        prefer this analysis over those with $m=5$? Explain why.

5.  *Number of imputations (continued)*. Continue with the data from the
    previous exercise.

    1.  Write an `R` function
        that automates the calculations of the previous exercise. Let
        `seed` run from 1 to 100 and let
        `m` take on values
        `m = c(3, 5, 10, 20, 30, 40, 50, 100, 200)`.

    2.  Plot the estimated proportions of explained variance due to
        missing data for the `age`-parameter
        against $m$. Based on this graph, how many imputations would you
        advise?

    3.  Check White’s conditions 1 and 2 (cf. Section \@ref(sec:howmany)).
        For which $m$ do these conditions true?

    4.  Does this also hold for categorical data? Use the
        `nhanes2` to study this.

6.  *Automated choice of $m$*. Write an
    `R` function that implements
    the methods discussed in Section \@ref(sec:howmany).
